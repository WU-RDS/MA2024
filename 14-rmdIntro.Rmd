

```{r, echo=FALSE, warning=FALSE}
library(knitr)
#rm(list = ls())
#This code automatically tidies code so that it does not reach over the page
opts_chunk$set(tidy.opts=list(width.cutoff=50),tidy=TRUE, rownames.print = FALSE, rows.print = 10)
```

# (PART) Assignments {-}

# R Markdown 


## Introduction to R Markdown

::: {.infobox .download data-latex="{download}"}
[You can download the example markdown file here](./Code/rmarkdown_example.Rmd)
:::

This page will guide you through creating and editing R Markdown documents. This is a useful tool for reporting your analysis (e.g. for homework assignments). Of course, there is also [a cheat sheet for R-Markdown](https://www.rstudio.org/links/r_markdown_cheat_sheet) and [this book](https://bookdown.org/yihui/rmarkdown/) contains a comprehensive discussion of the format. 

The following video contains a short introduction to the R Markdown format.

<br>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/o8FdyMAR-g4" frameborder="0" allowfullscreen></iframe>
</div>
<br>

### Creating a new R Markdown document {-}

In addition to the video, the following text contains a short description of the most important formatting options.  

Let's start to go through the steps of creating and .Rmd file and outputting the content to an HTML file. 

0. If an R-Markdown file was provided to you, open it with R-Studio and skip to [step 4](#step4) after adding your answers.

1. Open R-Studio

2. Create a new R-Markdown document
![](./rmdExplain/start.PNG)
![](./rmdExplain/openDoc.PNG)
![](./rmdExplain/enterName.PNG)
![](./rmdExplain/template.PNG)

3. Save with appropriate name
![](./rmdExplain/saving.PNG)

    3.1. Add your answers

    3.2. Save again

 <a name="step4"></a>
 
4. "Knit" to HTML 
![](./rmdExplain/knit.PNG)

5. Hand in appropriate file (ending in `.html`) on learn\@WU
![](./rmdExplain/handin.PNG)

### Text and Equations {-}

R-Markdown documents are plain text files that include both text and R-code. Using RStudio they can be converted ('knitted') to HTML or PDF files that include both the text and the results of the R-code. In fact this website is written using R-Markdown and RStudio. In order for RStudio to be able to interpret the document you have to use certain characters or combinations of characters when formatting text and including R-code to be evaluated. By default the document starts with the options for the text part. You can change the title, date, author and a few more advanced options. 

![First lines of an R-Markdown document](./rmdExplain/rmdHead.PNG)

The default is text mode, meaning that lines in an Rmd document will be interpreted as text, unless specified otherwise.

#### Headings {-}

Usually you want to include some kind of heading to structure your text. A heading is created using `#` signs. A single `#` creates a first level heading, two `##` a second level and so on. 

![](./rmdExplain/headings.PNG)

It is important to note here that the ```#``` symbol means something different within the code chunks as opposed to outside of them. If you continue to put a ```#``` in front of all your regular text, it will all be interpreted as a first level heading, making your text very large.

#### Lists {-}

Bullet point lists are created using `*`, `+` or `-`. Sub-items are created by indenting the item using 4 spaces or 2 tabs. 

````
* First Item
* Second Item
    + first sub-item
        - first sub-sub-item
    + second sub-item
````
* First Item
* Second Item
    + first sub-item
        - first sub-sub-item
    + second sub-item


Ordered lists can be created using numbers and letters. If you need sub-sub-items use `A)` instead of `A.` on the third level. 

````
1. First item
    a. first sub-item
        A) first sub-sub-item 
     b. second sub-item
2. Second item
````

1. First item
    a. first sub-item
        A) first sub-sub-item
    b. second sub-item
2. Second item


#### Text formatting {-}

Text can be formatted in *italics* (`*italics*`) or **bold** (`**bold**`). In addition, you can ad block quotes with `>`

````
> Lorem ipsum dolor amet chillwave lomo ramps, four loko green juice messenger bag raclette forage offal shoreditch chartreuse austin. Slow-carb poutine meggings swag blog, pop-up salvia taxidermy bushwick freegan ugh poke.
````
> Lorem ipsum dolor amet chillwave lomo ramps, four loko green juice messenger bag raclette forage offal shoreditch chartreuse austin. Slow-carb poutine meggings swag blog, pop-up salvia taxidermy bushwick freegan ugh poke.

### R-Code {-}

R-code is contained in so called "chunks". These chunks always start with three backticks and ```r``` in curly braces (``` ```{r} ```) and end with three backticks (``` ``` ```). Optionally, parameters can be added after the ```r``` to influence how a chunk behaves. Additionally, you can also give each chunk a name. Note that these have to be **unique**, otherwise R will refuse to knit your document.

#### Global and chunk options {-}

The first chunk always looks as follows


    ```{r setup, include = FALSE}`r ''`
    knitr::opts_chunk$set(echo = TRUE)
    ```

It is added to the document automatically and sets options for all the following chunks. These options can be overwritten on a per-chunk basis. 

Keep `knitr::opts_chunk$set(echo = TRUE)` to print your code to the document you will hand in. Changing it to `knitr::opts_chunk$set(echo = FALSE)` will not print your code by default. This can be changed on a per-chunk basis.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

    ```{r cars, echo = FALSE}`r ''`
    summary(cars)

    plot(dist~speed, cars)
    ```


```{r cars, echo = FALSE}
summary(cars)

plot(dist~speed, cars)
```

 
    ```{r cars2, echo = TRUE}`r ''`
    summary(cars)

    plot(dist~speed, cars)
    ```


```{r cars2, echo = TRUE}
summary(cars)

plot(dist~speed, cars)
```

A good overview of all available global/chunk options can be found [here](https://yihui.name/knitr/options/#chunk_options).

### LaTeX Math {-}

Writing well formatted mathematical formulas is done the same way as in [LaTeX](https://en.wikipedia.org/wiki/LaTeX). Math mode is started and ended using `$$`. 
````
$$
 f_1(\omega) = \frac{\sigma^2}{2 \pi},\ \omega \in[-\pi, \pi]
$$
````

$$
 f_1(\omega) = \frac{\sigma^2}{2 \pi},\ \omega \in[-\pi, \pi]
$$

(for those interested this is the spectral density of [white noise](https://en.wikipedia.org/wiki/White_noise))

Including inline mathematical notation is done with a single ```$``` symbol. 

````
${2\over3}$ of my code is inline.

````
${2\over3}$ of my code is inline.

<br>

Take a look at [this wikibook on Mathematics in LaTeX](https://en.wikibooks.org/wiki/LaTeX/Mathematics#Symbols) and [this list of Greek letters and mathematical symbols](https://www.sharelatex.com/learn/List_of_Greek_letters_and_math_symbols) if you are not familiar with LaTeX.

In order to write multi-line equations in the same math environment, use `\\` after every line. In order to insert a space use a single `\`. To render text inside a math environment use `\text{here is the text}`. In order to align equations start with `\begin{align}` and place an `&` in each line at the point around which it should be aligned. Finally end with `\end{align}`

````
$$
\begin{align}
\text{First equation: }\ Y &= X \beta + \epsilon_y,\ \forall X \\
\text{Second equation: }\ X &= Z \gamma + \epsilon_x
\end{align}
$$
````

$$
\begin{align}
\text{First equation: }\ Y &= X \beta + \epsilon_y,\ \forall X \\
\text{Second equation: }\ X &= Z \gamma + \epsilon_x
\end{align}
$$

#### Important symbols {-}

```{r, echo=FALSE, include=TRUE, results="asis", warning = FALSE}
library(knitr)
library(kableExtra)

lat <- readLines("./lat.txt")

lat1 <- paste0("$", lat, "$")
lat2 <- paste0("```", lat, "```")

mathy.df <- data.frame(Symbol = lat1, Code = lat2) 

kable(mathy.df, escape=FALSE) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

The `{}` after `_` and `^` are not strictly necessary if there is only one character in the sub-/superscript. However, in order to place multiple characters in the sub-/superscript they are necessary. 
e.g.


```{r, echo=FALSE, include=TRUE, results="asis", warning = FALSE}
lat <- readLines("./lat2.txt")

lat1 <- paste0("$", lat, "$")
lat2 <- paste0("```", lat, "```")

mathy.df <- data.frame(Symbol = lat1, Code = lat2) 

kable(mathy.df, escape=FALSE) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

#### Greek letters {-}

[Greek letters](https://en.wikipedia.org/wiki/Greek_alphabet#Letters) are preceded by a `\` followed by their name (`$\beta$` = $\beta$). In order to capitalize them simply capitalize the first letter of the name (`$\Gamma$` = $\Gamma$).

## Assignment 1

### Load libraries and data

For your convenience the following code will load the required `tidyverse` library as well as the data. Make sure to convert each of the variables you use for you analysis to the appropriate data types (e.g., `Date`, `factor`).

```{r  _a1, message=FALSE, warning=FALSE}
library(tidyverse)
music_data <- read.csv2("https://raw.githubusercontent.com/WU-RDS/MA2024/main/data/music_data_fin.csv")
str(music_data)
head(music_data, 2) 
```

### Task 1

1. Determine the most popular song by the artist "BTS".
2. Create a new `data.frame` that only contains songs by "BTS" (Bonus: Also include songs that feature both BTS and other artists, see e.g., "BTS feat. Charli XCX")
3. Save the `data.frame` sorted by success (number of streams) with the most popular songs occurring first.

```{r question_1_a1}
# provide your code here 
## 1.
music_data |>
  filter(artistName == "BTS") |>
  slice_max(order_by = streams, n = 1) |>
  select(artistName, trackName)
## 2.
bts_data <- music_data |>
  filter(str_detect(artistName, "BTS")) |>
  arrange(desc(streams))
distinct(bts_data, artistName)
head(bts_data)
tail(bts_data)
```

### Task 2

Create a new `data.frame` containing the 100 most streamed songs. 

```{r question_2_a1}
# provide your code here 
top100 <- slice_max(music_data, order_by = streams, n = 100)
range(top100$streams)
```

### Task 3

1. Determine the most popular genres. 
    
- Group the data by genre and calculate the total number of streams within each genre. 
- Sort the result to show the most popular genre first.

2. Create a bar plot in which the heights of the bars correspond to the total number of streams within a genre (Bonus: order the bars by their height)


```{r question_3_a1}
# provide your code here 
genre_data <- music_data |>
  summarize(.by = genre, total_streams = sum(streams)) |>
  arrange(desc(total_streams)) |>
  mutate(genre = fct_reorder(as.factor(genre), total_streams))
head(genre_data)
ggplot(genre_data, aes(y = genre, x = total_streams)) +
  geom_bar(stat="identity")
```


### Task 4

1. Rank the music labels by their success (total number of streams of all their songs)
2. Show the total number of streams as well as the average and the median of all songs by label. (Bonus: Also add the artist and track names and the number of streams of each label's top song to the result)


```{r question_4_a1}
# provide your code here 
label_data <- music_data |>
  summarize(
    .by = label, 
    total_streams = sum(streams), 
    avg_streams = mean(streams), 
    med_streams = median(streams), 
    top_song_artist = artistName[which.max(streams)],
    top_song_title = trackName[which.max(streams)],
    top_song_streams = max(streams)
    ) |>
  mutate(label_rank = dense_rank(-total_streams)) |>
  arrange(desc(total_streams))
label_data
```


### Task 5

1. How do genres differ in terms of song features (audio features + song length + explicitness + song age)?

- Select appropriate summary statistics for each of the variables and highlight the differences between genres using the summary statistics.
- Create an appropriate plot showing the differences of "energy" across genres.
   

```{r question_5_a1}
# provide your code here
music_data |>
  summarize(.by = genre, across(danceability:explicit, 
                                list("avg" = mean,
                                     "std.dev" = sd,
                                     "median" = median, 
                                     "pct_10" = \(x) quantile(x, 0.1), 
                                     "pct_90" = \(x) quantile(x, 0.9))
                                )
            )
ggplot(music_data, aes(x = fct_reorder(factor(genre), energy, median), y = energy)) +
    geom_boxplot() +
    theme(axis.title.x = element_blank()) +
    scale_x_discrete(guide = guide_axis(n.dodge=2))

```


### Task 6

Visualize the number of songs by label. 

```{r question_6_a1}
# provide your code here 
music_data |>
  summarize(.by = label, n_songs = n_distinct(isrc)) |>
  mutate(label = fct_reorder(as.factor(label), n_songs)) |>
  ggplot(aes(y = label, x = n_songs)) +
    geom_bar(stat = "identity")
```


### Task 7

Visualize the average monthly artist listeners (`monthly_listeners_artist`) by genre.

```{r question_7_a1}
# provide your code here 
music_data |>
  summarize(.by = genre, avg_m_listeners = mean(monthly_listeners_artist)) |>
  mutate(genre = fct_reorder(factor(genre), avg_m_listeners)) |>
ggplot(aes(x = avg_m_listeners, y = genre)) +
  geom_bar(stat = "identity") +
  labs(x = "Average monthly artist listeners")
```

### Task 8

Create a histogram of the variable "valence".

```{r question_8_a1}
# provide your code here 
ggplot(music_data, aes(x = valence)) +
  geom_histogram()
```

### Task 9

Create a scatter plot showing `youtube_views` and `shazam_counts` (Bonus: add a linear regression line). Interpret the plot briefly.

```{r question_9_a1}
# provide your code here 
ggplot(music_data, aes(x = youtube_views, y = shazam_counts)) +
  geom_point() +
  geom_smooth(method = "lm")
```

On average Youtube views and Shazam counts show a positive coefficient in the linear regression. However, the relationship does not appear to be linear.

## Assignment 2


This assignment consists of four parts. When answering the questions, please remember to address the following points, where appropriate:

* Formulate the corresponding hypotheses and choose an appropriate statistical test
* Provide the reason for your choice and discuss if the assumptions of the test are met 
* Convert the variables to the appropriate type (e.g., factor variables)
* Create appropriate graphs to explore the data (e.g., plot of means incl. confidence intervals, histogram, boxplot)
* Provide appropriate descriptive statistics for the variables (e.g., mean, median, standard deviation, etc.)
* Report and interpret the test results accurately (including confidence intervals)  
* Finally, don't forget to report your research conclusion

When you are done with your analysis, click on "Knit to HTML" button above the code editor. This will create a HTML document of your results in the folder where the "assignment2.Rmd" file is stored. Open this file in your Internet browser to see if the output is correct. If the output is correct, submit the HTML file via Learn\@WU. The file name should be "assignment2_studendID_lastname.html".

### Assignment 2a

As a mobile app product manager, you are looking for ways to improve user engagement and in-app purchases. Your team has launched an A/B test to analyze the effect of a new user interface (UI) feature. You have data that contains information about user behavior within your app.

The data file contains the following variables:

* **userID:** Unique user ID.
* **exp_group:** Experimental group (indicator variable w/ 2 levels: 0 = control, 1 = treatment).
* **in_app_purchases:** Total amount spent by a user in the app in the past month (in USD).
* **time_in_app:** Average time a user spends in your app per session (in minutes).

Use R and appropriate methods to answer the following questions:

1. The finance department asks you to provide an estimate of the average amount spent by users through in-app purchases. Compute the 95% confidence interval for the mean amount spent and provide an interpretation of the interval.
2. You run an A/B test to analyze the effect of a new UI feature on in-app purchases and time spent in the app. The information regarding which group a user has been assigned to is stored in the variable "exp_group". Is there a significant difference regarding in-app purchases and time spent between users from the control and treatment groups? Please include the effect size (Cohen's d) and confidence intervals in your report.
3. Assume that you plan to run an experiment to test two different notification strategies. You randomly assign app users to the control and experimental conditions. How many users would you need to include in each group if you assume the effect size to be 0.1 for a significance level of 0.05 and power of 0.8?

### Data analysis

### Load data

```{r}
app_user_data <- read.table("https://raw.githubusercontent.com/WU-RDS/MA2024/main/user_data_q1.csv", 
                          sep = ",", 
                          header = TRUE) #read in data
head(app_user_data)
str(app_user_data)
```


### Question 1

First, we take a look at the statistics for the amount spent for in app purchases our customers make and at the graph with the distribution of purchases made in app per customer.

To compute the confidence interval for the average customer, we will need the mean, the standard error and the critical value for a t-distribution (because we don't know exactly the variance in the population).

```{r, echo = FALSE}
options(conflicts.policy = list(warn = FALSE))
```

```{r libs, warnings = FALSE, messages=FALSE}
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(psych))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggstatsplot))
```

```{r question_1_1}
#First let's have a look at the purchases in the app in the data
psych::describe(app_user_data$in_app_purchases)

ggplot(app_user_data, aes(in_app_purchases)) + 
  geom_histogram(col = "white", fill = "lavenderblush3", bins = 50) + 
  geom_vline(data = app_user_data %>% dplyr::summarise(mean = mean(in_app_purchases)),aes(xintercept = mean), 
             linewidth = 0.7, color = "gray19") +
  labs(x = "Purchases", y = "Frequency") + 
  ggtitle("Distribution of purchases per customer") +
  theme_minimal()

# Compute mean, standard error, and confidence interval for in-app purchases
mean_purchases <- mean(app_user_data$in_app_purchases)
sd_purchases <- sd(app_user_data$in_app_purchases)
n <- nrow(app_user_data)
se_purchases <- sd_purchases / sqrt(n)
df <- n - 1
t_crit <- qt(0.975, df)

# Confidence Interval
ci_lower <- mean_purchases - t_crit * se_purchases
ci_upper <- mean_purchases + t_crit * se_purchases

print(ci_lower)
print(ci_upper)

# Alternatively: get confidence interval from t.test
t.test(app_user_data$in_app_purchases)$conf.int
```


The confidence interval for in app purchases is CI =  [10.87;11.36]
Interpretation: If we take 100 samples and calculate the mean and confidence interval for each of them, then the true population mean would be included in 95% of these intervals. In the sample we have, this interval spans from 10.87 to 11.36.


### Question 2


We need to analyze if a new UI feature has an effect on in app purchases. We need to formulate the null hypothesis as the first step. In our case the null hypothesis is that the new UI feature has no effect on the mean in-app purchases, that there is no difference in the mean in-app purchases between two populations. The alternative hypothesis states that the new UI feature has an effect on the mean in-app purchases, meaning that there is a difference in the mean in-app purchases between the two populations.

We first transform the variable exp_group into a factor and inspect the data with descriptive statistics. It can be already seen that the mean of in app-purchases is higher in the treatment group. Next we visualize the data, for this we can use a boxplot. 

To test whether or not this difference is significant, we need to use an independent-means t-test, since we have different customers in each group, meaning that we have collected the data using a between-subjects design (the customers in one condition are independent of the customers in the other condition). The requirements are met:
1) the DV (in-app purchases) is measured on a ratio scale; 2) there are more than 30 observations in each group, so the data is normally distributed according to Central Limit Theorem; 3) the feature was assigned randomly, so the groups are independent; 4) Welch's t-test corrects for unequal variance.

We also then calculate the effect size (Cohen's d). Then we can also visualize the results of the test.



```{r question_2_1}
# Load necessary libraries
library(ggplot2)
library(data.table)
library(lsr)
library(pwr)
library(psych)

#making IV a factor
app_user_data$exp_group <- as.factor(app_user_data$exp_group)

#looking at descriptive statistics
describeBy(app_user_data$in_app_purchases, app_user_data$exp_group) #description of control and treatment groups

# Boxplot for In-App Purchases by Experimental Group
ggplot(app_user_data, aes(x = factor(exp_group), y = in_app_purchases, fill = factor(exp_group))) +
  geom_boxplot() +
  labs(title = "Boxplot of In-App Purchases by Experimental Group",
       x = "Group (0 = Control, 1 = Treatment)",
       y = "In-App Purchases (USD)") +
  scale_fill_manual(values = c("lightblue", "lightgreen")) +
  theme_minimal()


# t-test for differences in in-app purchases
t_test_purchases <- t.test(in_app_purchases ~ exp_group, data = app_user_data)
print(t_test_purchases)


# Compute Cohen's d for in app purchases
cohen_d_purchases <- cohensD(in_app_purchases ~ exp_group, data = app_user_data)
print(cohen_d_purchases)


#Visualization of test results
ggbetweenstats(
  data = app_user_data,
  plot.type = "box",
  x = exp_group, #2 groups
  y = in_app_purchases ,
  type = "p", #default
  effsize.type = "d", #display effect size (Cohen's d in output)
  messages = FALSE,
  bf.message = FALSE,
  mean.ci = TRUE,
  title = "Average revenue per customer by group"
) 


```

Interpretations: As we can see from the descriptive statistics and the plot for control and treatment groups, the in-app purchases are higher in the group that was exposed to the new UI feature. The t-test showed significant result because the p-value is smaller than 0,05, meaning that we can reject the null hypothesis that there is no difference in the mean of the in-app purchases. The p-value states that the probability of finding a difference of the observed magnitude or higher, if the null hypothesis was in fact true (if there was in fact no difference between the populations). For us it means that the new UI feature in fact has an effect on the average in-app purchases. Also: Since 0 (the hypothetical difference in means from H0) is not included in the interval, it confirms that we can reject the null hypothesis. The Cohen's d effect size value of 0.4234 suggests that the effect of the new UI feature is small to medium.

The plot shows us that in app purchases are higher in the treatment group (Mean = 12.15) compared to the control group (Mean = 10.08). This means that, on average, the in app purchases were 2.07 higher in the treatment group, compared to the control group. An independent-means t-test showed that this difference is significant: t(1597) = 8.47, p < .05 (95% CI = [1.59, 2.55]); effect size is small to medium = 0.42.



Now we can look at the influence of the new UI feature on the time spent in app.

First, we need to formulate the null hypothesis. In this case the null hypothesis is that the new UI feature has no effect on the mean time spent in app, that there is no difference in the mean time spent in app between two populations. The alternative hypothesis states that the new UI feature has an effect on the mean time spent in app, meaning that there is a difference in the mean time in app between the two populations. 

First, we inspect the data with descriptive statistics. It can be already seen that the mean of in app purchases is slightly higher in the treatment group. Next we visualize the data, for this we can use a boxplot. 

We can use the independent means t-test because again the necessary assumptions are met: 1) The dependent variable (time in app) is measured on an ratio scale; 2) We have more than 30 observations per group; 3) The groups are independent.

We also then calculate the effect size (Cohen's d). Then we can also visualize the results of the test.


```{r}

describeBy(app_user_data$time_in_app, app_user_data$exp_group)

# Boxplot for Time Spent in the App
ggplot(app_user_data, aes(x = factor(exp_group), y = time_in_app, fill = factor(exp_group))) +
  geom_boxplot() +
  labs(title = "Boxplot of Time Spent in App by Experimental Group",
       x = "Group (0 = Control, 1 = Treatment)",
       y = "Time Spent in App (minutes)") +
  scale_fill_manual(values = c("lightblue", "lightgreen")) +
  theme_minimal()

# t-test for differences in time spent in app
t_test_time_in_app <- t.test(time_in_app ~ exp_group, data = app_user_data)
print(t_test_time_in_app)

# Compute Cohen's d for in app purchases
cohen_d_time_in_app <- cohensD(time_in_app ~ exp_group, data = app_user_data)
print(cohen_d_time_in_app)

#Visualization of test results
ggbetweenstats(
  data = app_user_data,
  plot.type = "box",
  x = exp_group, #2 groups
  y = time_in_app ,
  type = "p", #default
  effsize.type = "d", #display effect size (Cohen's d in output)
  messages = FALSE,
  bf.message = FALSE,
  mean.ci = TRUE,
  title = "Average revenue per customer by group"
) 


```

Interpretations: From the descriptive statistics and the boxplot, we can already see that the mean time spent in app is higher in the treatment group. However, we need to conduct the test to see if this result is significant. The t-test showed significant result because the p-value is smaller than 0,05, meaning that we can reject the null hypothesis that there is no difference in the mean of time spent in app. The p-value states that the probability of finding a difference of the observed magnitude or higher, if the null hypothesis was in fact true (if there was in fact no difference between the populations). For us it means that the new UI feature in fact has an effect on the average time spent in app. Also: Since 0 (the hypothetical difference in means from H0) is not included in the interval, it confirms that we can reject the null hypothesis. The Cohen's d effect size value of 0.387 suggests that the effect of the new UI feature is small to medium.

The plot shows us that time spent in app is higher in the treatment group (Mean = 21.92) compared to the control group (Mean = 19.99). This means that, on average, the time spent in app was 1.93 higher in the treatment group, compared to the control group. An independent-means t-test showed that this difference is significant: t(1598) = 7.74, p < .05 (95% CI = [1.45, 2.43]); effect size is small to medium = 0.387.


### Question 3

To define the number of users that should be placed in two different conditions, pwr.t.test() function should be used. If the goal of the experiment is to simply detect significant difference between the groups, the sample size definition should be based on two-sided test.

Given the effect size = 0.1, significance level = 0.05, and power = 0.8, sample size for each group will be:


```{r a2_question_3_1}
# provide your code here (you can use multiple code chunks per question if you like)

# Power analysis for sample size calculation
sample_size <- pwr.t.test(d = 0.1, sig.level = 0.05, power = 0.8, type = "two.sample")
print(sample_size)

```

To achieve our desired effect size of 0.1, a significance level of 0.5 and a power of 0.8 we would need to include at least 1571 customers per group in the experiment.

### Assignment 2b

After conducting the experiment described above, you would like to find out whether push notifications can further improve user engagement with your mobile app. You expose a set of users, who were already exposed to the new UI feature, to push notifications and record the time they spend in the app before and after implementing the notifications.

You obtain a new data set with the following variables:

* **userID:** Unique user ID.
* **time_in_app_1:** Average time (in minutes) a user spends in your app per session before receiving push notifications.
* **time_in_app_2:** Average time (in minutes) a user spends in your app per session after receiving push notifications.

Use R and appropriate methods to answer the following question:

4. Did the push notifications lead to a significant increase in the time that users spend in the app compared to before the notifications were implemented ? Conduct an appropriate statistical test to determine if the difference is statistically significant. Please include the effect size (Cohen's d) and the confidence intervals in your report.

### Data analysis

### Load data

```{r a2_load_data_2234}
app_user_data_time <- read.table("https://raw.githubusercontent.com/WU-RDS/MA2024/main/user_data_q2.csv", 
                          sep = ",", 
                          header = TRUE) #read in data
head(app_user_data_time)
str(app_user_data_time)
```
### Question 4

We want to examine if push notifications have an effect on average time a user spends in the app. The null hypothesis here is that there is no difference in the mean time spent in the app for the same customers between the presence and absence of push notifications. Because the observations come from the same population of customers (a within-subject design), we refer to the difference in the means for the same population when stating our hypotheses. The alternative hypothesis states that there is a difference between the time in app for the same customers.

We start our analysis with looking at the descriptive statistics and at the plot. Then we conduct a dependent means t-test to see if the difference is significant.


```{r a2_question_4_1}
# provide your code here (you can use multiple code chunks per question if you like)

suppressPackageStartupMessages(library(Rmisc))
library(tidyr)

#Descriptive statistics
psych::describe(app_user_data_time[!is.na(app_user_data_time$time_in_app_2), c("time_in_app_1","time_in_app_2")])

#Boxplot

time_data <- app_user_data_time |>
  drop_na(time_in_app_2) |>
  select(time_in_app_1, time_in_app_2) |>
  pivot_longer(
    cols = c(time_in_app_1, time_in_app_2), 
    names_to = "push_notifications", values_to = "time_in_app")

ggplot(time_data, aes(x = push_notifications, y = time_in_app)) + geom_boxplot() +
    geom_jitter(alpha = 0.2, color = "lavenderblush4") + 
    labs(x = "", y = "Time spent in app", title = "Boxplot of time in app by group") + 
    theme_minimal()


# Paired t-test for time spent in app before and after push notifications
t_test_result <- t.test(app_user_data_time$time_in_app_2, app_user_data_time$time_in_app_1, paired = TRUE)
print(t_test_result)

# Compute Cohen's d for paired samples
cohen_d_result <- cohensD(app_user_data_time$time_in_app_2, app_user_data_time$time_in_app_1, method = "paired")
print(cohen_d_result)

#Visualization of the test
ggwithinstats(
  data = time_data,
  x = push_notifications,
  y = time_in_app,
  path.point = FALSE,
  path.mean = TRUE,
  title = "Time on site with or without push notifications",
  messages = FALSE,
  bf.message = FALSE,
  mean.ci = TRUE,
  effsize.type = "d" # display effect size (Cohen's d in output)
)


```

Interpretation: It appears that there is a difference in the means from the descriptive statistics and the plots. To test whether it is significant, we need to run a t-test. This time we need a different version of the t-test because the same customers are observed for the app with and without push notifications (the same customers are shown both versions of the app). This means that we need a dependent means t-test, or paired samples t-test. The other assumptions are identical to the independent-means t-test. 

The p-value is lower than the significance level of 5% (p < .05), which means that we can reject the null hypothesis that there is no difference in the mean time in app between absence and presence of push notifications. The confidence interval confirms the conclusion to reject the null hypothesis since 0 is not contained in the range of plausible values.

The Cohen's d effect size of 0.2857 shows us that the effect is rather small.

The results of the experiment show that, on average, the same users used the app on average 2.03 minutes longer when it included the push notifications (Mean = 27.24) compared to the app without the push notifications (Mean = 25.21). This difference was significant: t(416) = 5.83, p < .05 (95% CI = [1.35, 2.72]); effect size is small = 0.29.

This means that it makes sense to include push notifications to the app as standard practice.




### Assignment 2c

As a marketing analyst for an online retailer, you're tasked with evaluating how different levels of GDPR-compliant behavioral targeting affect purchase behavior. Given the restrictions imposed by GDPR on using personal data, the retailer conducts an experiment with three levels of targeting:

1. No targeting: Users receive no targeted ads (default ad experience).
2. Segment-based targeting: Ads are tailored using aggregate-level data (e.g., based on product categories users browse, not their personal data).
3. Individual personalized behavioral targeting: Ads are personalized based on the specific behavior of individual users (using compliant first-party data).

You obtain a data set with the following variables:

* **customerID:** Unique customer ID.
* **revenue:** Total revenue generated by the customer during the experiment (in USD).
* **satisfaction:** Customer satisfaction score from a post-purchase survey (measured in 11 categories from 0 [very dissatisfied] to 10 [very satisfied]).
* **targeting:** Type of targeting the customer was exposed to (1 = no targeting, 2 = segment-based targeting, 3 = personalized behavioral targeting).

Use R and appropriate methods to answer the following question:

5. Are there significant differences in revenue between the three targeting strategies? 
6. Did the targeting strategy significantly influence customer satisfaction? 

### Data analysis

### Load data

```{r a2_load_data_2343}
targeting_data <- read.table("https://raw.githubusercontent.com/WU-RDS/MA2024/main/user_targeting_data.csv", 
                          sep = ",", 
                          header = TRUE) #read in data
head(targeting_data)
str(targeting_data)
```

### Question 5

To answer the question of whether the type of targeting has an effect on revenue, we need to formulate the null hypothesis first. In this case, the null hypothesis is that the average level of sales is equal for all three targeting types. The alternative hypothesis states that mean revenue is not equal among three targeting types. 

The appropriate test for such a hypothesis is one-way ANOVA since we have a metric-scaled dependent variable and a categorical independent variable with more than two levels. First, we need to recode the independent variable into factor. Next we take a look at descriptive statistics for the data and create appropriate plots. 

Before we move to the formal test, we need to see if a series of assumptions are met, namely: 1) Independence of observations; 2) Distributional assumptions; 3) Homogeneity of variances.

Due to the fact that there are more than 30 observations in each group we can rely on the Central Limit Theorem to satisfy the distributional assumptions. We can still test this assumption using Shapiro-Wilk normality test and Q-Q plots. Homogeneity of variances can be checked with Levene’s test.

After checking that the assumptions are met, we can proceed with ANOVA and show also the plot for the test. Next we will briefly inspect the residuals of the ANOVA to see if the assumptions of the test really are justified.

The ANOVA result only tells us that the means of the three groups are not equal, but it does not tell us anything about which pairs of means are unequal. To find this out we need to conduct a post-hoc test.


```{r a2_question_5_1}
# provide your code here (you can use multiple code chunks per question if you like)


targeting_data$targeting <- factor(targeting_data$targeting, 
                                   levels = c("Personalized Targeting", 
                                              "Segment-Based Targeting", 
                                              "No Targeting"))

#Descriptive statistics
describeBy(targeting_data$revenue, targeting_data$targeting)


#Visual inspection of data

ggplot(targeting_data,aes(x = targeting, y = revenue)) + 
  geom_boxplot() +
  geom_jitter(colour="red", alpha = 0.1) +
  theme_bw() +
  labs(x = "Targeting", y = "Revenue")+
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5,color = "#666666")) 

#Distributional assumptions - checking for normal distributions

#test for normal distribution of variables - Shapiro-Wilk test
by(targeting_data$revenue, targeting_data$targeting, shapiro.test)

qqnorm(targeting_data[targeting_data$targeting == "Personalized Targeting", ]$revenue) 
qqline(targeting_data[targeting_data$targeting == "Personalized Targeting", ]$revenue)

qqnorm(targeting_data[targeting_data$targeting == "Segment-Based Targeting", ]$revenue) 
qqline(targeting_data[targeting_data$targeting == "Segment-Based Targeting", ]$revenue)

qqnorm(targeting_data[targeting_data$targeting == "No Targeting", ]$revenue) 
qqline(targeting_data[targeting_data$targeting == "No Targeting", ]$revenue)

#Homogeneity of variance

library(car)
leveneTest(revenue ~ targeting, data = targeting_data, center = mean)


# Perform ANOVA for revenue across targeting strategies
anova_result <- aov(revenue ~ targeting, data = targeting_data)
summary(anova_result)

#Visualize the test
library(ggstatsplot)
ggbetweenstats(
  data = targeting_data,
  x = targeting,
  y = revenue,
  plot.type = "box",
  pairwise.comparisons = TRUE,
  pairwise.annotation = "p.value",
  p.adjust.method = "bonferroni",
  effsize.type = "eta", #if var.equal = FALSE, returns partial eta^2 
  var.equal = TRUE,
  mean.plotting = TRUE, 
  mean.ci = TRUE, 
  mean.label.size = 2.5,
  type = "parametric", 
  k = 3, 
  outlier.label.color = "darkgreen", 
  title = "Comparison of revenues between groups",
  xlab = "Experimental group", 
  ylab = "Revenue", 
  messages = FALSE,
  bf.message = FALSE,
)

#Inspect the residuals
plot(anova_result,1)
plot(anova_result,2)

shapiro.test(resid(anova_result))

#Effect size

summary(anova_result)[[1]]$"Sum Sq"[1]/(summary(anova_result)[[1]]$"Sum Sq"[1] +
    summary(anova_result)[[1]]$"Sum Sq"[2])


# Tukey's post-hoc test for pairwise comparisons
tukey_result <- TukeyHSD(anova_result)
print(tukey_result)

plot(tukey_result)


# Boxplot for Revenue by Targeting Strategy
ggplot(targeting_data, aes(x = targeting, y = revenue, fill = targeting)) +
  geom_boxplot() +
  labs(title = "Boxplot of Revenue by Targeting Strategy",
       x = "Targeting Strategy",
       y = "Revenue (USD)") +
  scale_fill_manual(values = c("lightblue", "lightgreen", "lightcoral")) +
  theme_minimal()






```

Both the summary statistics and the plot show that the means are not equal among the three groups. Especially the difference between personalized targeting and no targeting seem to be quite high. 

First we check if the assumptions of ANOVA are met. The insignificant result of Shapiro-Wilk test shows that we cannot reject the null hypothesis that the residuals are normally distributed. The same we can see from the Q-Q plots.

The null hypothesis of Levene's test is that the variances are equal, with the alternative hypothesis being that the variances are not all equal. The insignificant result of this test demonstrates that variances are equal, so this assumption is also met. 

The ANOVA showed a p-value lower that 0.05, which means we can reject the null hypothesis that the mean revenue is the same for all three groups with different types of targeting. 

The Q-Q plots show us that the residuals are equally distributed, which is confirmed also by the insignificant result of the Shapiro-Wilk test. The null hypothesis for this test is that the distribution of residuals is normal. 

According to the test, the effect of different types of targeting on revenues was detected: F = 41.9, p < 0.05, with the effect size which is rather small η2 = 0.22. 

The Tukey's HSD test compares pairwise all three groups and we can see from the result that we can reject null hypothesis in all three case, which means that the revenue means are all significantly different from each other. It is clearly visible that none of the CIs cross the 0 bound, which further indicates that all differences in means are statistically significantly different from 0.

From a reporting standpoint we can say that revenue is higher when using personalized targeting. This means that personalized targeting helps us to increase sales and should thus be the preferred choice.

### Question 6

For this question we want to examine whether the customer satisfaction is significantly different for the groups with three different types of targeting. Because we are dealing with data on an ordinal scale, we cannot use ANOVA for this analysis. The non-parametric counterpart is the Kruskal-Wallis test, which tests for differences in medians between more than two groups. Hence, the null hypothesis is that the medians are equal in each group, and the alternative hypothesis is that there is a difference in medians.

First, we inspect the descriptive statistics and the plot. The only assumption for Kruskal-Wallis test is that the DV has to be at least ordinal scaled, and this assumption is met. 


```{r question_6_1}
# provide your code here (you can use multiple code chunks per question if you like)

#Descriptive statistics by for customer satisfaction by group
describeBy(targeting_data$satisfaction, targeting_data$targeting)

# Boxplot for Satisfaction by Targeting Strategy
ggplot(targeting_data, aes(x = targeting, y = satisfaction, fill = targeting)) +
  geom_boxplot() +
  labs(title = "Boxplot of Customer Satisfaction by Targeting Strategy",
       x = "Targeting Strategy",
       y = "Satisfaction Score") +
  scale_fill_manual(values = c("lightblue", "lightgreen", "lightcoral")) +
  theme_minimal()

# Kruskal-Wallis test for satisfaction across targeting strategies
kruskal_result <- kruskal.test(satisfaction ~ targeting, data = targeting_data)
print(kruskal_result)

ggbetweenstats(
  data = targeting_data,
  plot.type = "box",
  x = targeting, #3 groups
  y = satisfaction,
  type = "nonparametric",
  pairwise.comparisons = TRUE,
  pairwise.annotation = "p.value",
  p.adjust.method = "bonferroni",
  messages = FALSE,
  title = "Median satisfaction for different targeting groups"
)


```

We can see from the descriptive statistics and from the boxplot that the median customer satisfaction for customers with personalized targeting is slightly higher (=6) than for other two targeting types (=5). 

The p-value of Kruskal-Wallis test is higher than 0.05, which indicates that we cannot reject the null hypothesis. This means that medians of customer satisfaction are not different among the targeting groups. 


### Assignment 2d

As a digital marketing manager, you want to evaluate the effectiveness of a new email subscription pop-up feature designed to increase newsletter signups. You run an A/B test where some visitors to your website see the new subscription pop-up, while others experience the regular sign-up option without a pop-up. Your goal is to compare the conversion rate (whether visitors signed up for the newsletter) between the control group (no pop-up) and the treatment group (pop-up).

You obtain a new data set with the following variables:

* **customerID:** Unique customer ID.
* **conversion:** Indicator variable for whether a visitor signed up for the newsletter (0 = no, 1 = yes).
* **exp_group:** Experimental group (0 = control, no pop-up; 1 = treatment, pop-up).

7. Did the new email subscription pop-up have a significant effect on the conversion rate? 

### Data analysis

### Load data

```{r load_data234_4}
conversion_data <- read.table("https://raw.githubusercontent.com/WU-RDS/MA2024/main/conversion_data.csv", 
                          sep = ",", 
                          header = TRUE) #read in data
head(conversion_data)
str(conversion_data)
```
### Question 7

To find out if a new email subscription pop-up feature has an effect on the conversion rate, we can use a test for proportions. To test for the equality of proportions (and therefore no difference between them) we can use a Chi-squared test.

Our null hypothesis in this case states that the proportions of conversion are the same for groups with and without the subscription pop-up feature. Our alternative hypothesis states that these proportions are unequal. First, we have to recode the relevant variables into factors. Then we create a contingency table and a plot to take a look at the proportions of conversion rates in the control and treatment groups. We can then conduct the formal Chi-squared test to see if the difference in conversion rates is significant.

```{r question_7_1}
# provide your code here (you can use multiple code chunks per question if you like)

#Recoding variables into factors
conversion_data$exp_group <- factor(conversion_data$exp_group, 
                                   levels = c("Control", 
                                              "Treatment"))

conversion_data$conversion <- factor(conversion_data$conversion, levels = c(0,1), labels = c("no", "yes"))

# Create a contingency table for conversions
conversion_table <- table(conversion_data$exp_group, conversion_data$conversion)
print(conversion_table)
prop.table(conversion_table, 1)

#Visualization
rel_freq_table <- as.data.frame(prop.table(table(conversion_data$exp_group, conversion_data$conversion), 1))
names(rel_freq_table) <- c("exp_group", "conversion","freq") # changing names of the columns
rel_freq_table
ggplot(rel_freq_table, aes(x = exp_group, y = freq, fill = conversion)) + #plot data
  geom_col(width = .7) + #position
  geom_text(aes(label = paste0(round(freq*100,0),"%")), position = position_stack(vjust = 0.5), size = 4) + #add percentages
  ylab("Proportion of conversions") + xlab("Exp group") + # specify axis labels
  theme_bw()


# Proportion test to compare conversion rates between groups
prop_test_result <- prop.test(conversion_table)
print(prop_test_result)

table <- table(conversion_data$conversion,conversion_data$exp_group)
chisq.test(table, correct = TRUE)

#effect size
test_stat <- chisq.test(conversion_table, correct = FALSE)$statistic
n <- nrow(conversion_data)
phi1 <- sqrt(test_stat/n)
phi1


```

We can see in the contingency table and in the plot that the conversion rate in the treatment group of 29% is higher than the conversion rate of 19% in the control group. To see if this difference is significant, we have to conduct the formal chi-squared test. It can be clearly seen from the test that p-value is lower than 0.05, so the result of the treatment on the conversion rate is statistically significant. We also calculated the effect size Phi: it is pretty small 0.128.

From the managerial perspective, it makes sense to include the new email subscription pop-up feature since it significantly increases the coversion rate, although the effect size is rather small.

## Assignment 3

### Assignment A

As a marketing manager at a consumer electronics company, you have been assigned the task of evaluating the effectiveness of various marketing activities on the sales of smart home devices (smart speakers). The company wants to understand the relative influence of leaflet promotions, in-store sales representatives, and radio advertising, as well as the impact of pricing on overall sales.

The dataset provided contains data from multiple stores over the past year. Each row in the dataset corresponds to a different store, detailing the sales performance of smart home devices alongside the store’s marketing investments and product pricing.

The following variables are available to you:

* Sales: Number of units sold in each store
* Price: Sale price of the product (in Euros)
* Marketing Contribution: Promotion costs for including the product in leaflets distributed by the store (in Euros)
* Sales Reps: Expenditure on in-store promotions managed by the company’s sales representatives (in Euros)
* Retail Media POS: Advertising expenses for marketing placements at the point of sale (in Euros)
* Region: Categorical variable indicating the region where the store is located (rural, suburban, or urban)

**Task Instructions**

Please conduct the following analyses:

1. Regression Equation: Formally state the regression equation you will use to determine the relative influence of each marketing activity and price on sales. Save the equation as a "formula" object in the R code chunk.

2. Variable Description: Describe the model variables using appropriate summary statistics and visualizations to provide a clear understanding of each variable’s distribution and scale.

3. Multiple Linear Regression Model: Estimate a multiple linear regression model to evaluate the relative influence of each variable. Before interpreting the results, assess whether the model meets the assumptions of linear regression. Use appropriate diagnostic tests and plots to evaluate these assumptions.

4. Alternative Model Specifications: If the model assumptions are not fully met, evaluate alternative model specifications using different functional forms. Justify your choice of model based on diagnostic criteria and model properties.

5. Model Interpretation: Interpret the results of the chosen model:

* Which variables have a significant influence on sales, and what do the coefficients imply about each variable’s effect?
* What is the relative importance of each predictor?
* Interpret the F-test results.
* How would you assess the fit of the model? Include a visualization of model fit for clarity.

6. Sales Prediction: Based on your chosen model, predict the sales quantity for a store planning the following marketing activities: Price: €350, Marketing Contribution: €10,000, Sales Reps: €6,000, Retail Media POS: €3,000. Provide the equation you used for the prediction.

7. Assess to what extent customers from different regions differ regarding their price sensitivity. Are customers from urban and suburban areas more or less price-sensitive compared to rural areas? Provide a detailed explanation of your analysis and results.

When you have completed your analysis, click the “Knit to HTML” button above the code editor. This will generate an HTML document of your results in the folder where the "assignment3.Rmd" file is stored. Open the HTML file in your Internet browser to verify the output. Once verified, submit the HTML file via Canvas. Name the file: “assignment3_studentID_name.html”.

### Data analysis

### Load data

```{r load_d234ata}
sales_data <- read.csv("https://raw.githubusercontent.com/WU-RDS/MA2024/main/data/assignment3.csv", 
                          header = TRUE) #read in data
head(sales_data)
str(sales_data)

library(tidyverse)
library(psych)
library(Hmisc)
library(ggstatsplot)
library(ggcorrplot)
library(car)
library(lmtest)
library(lm.beta)
options(scipen = 999)
set.seed(123)


```

### Question 1



In a first step, we specify the regression equation. In this case, sales of home devices is the dependent variable, and predictors for store i are 1) price, 2) promotion costs, 3) expenditures on sales representatives, 4) costs for marketing placements at POS.


$$
Sales_i=\beta_0 + \beta_1 * Price_i + \beta_2 * MarketingContribution_i + \beta_3 * SalesReps_i + \beta_4 * RetailMediaPOS_i + \epsilon_i
$$

This equation will be used later to turn the output of the regression analysis (namely the coefficients: β0 - intersect coefficient, and β1, β2, and β3 that represent the unknown relationship between sales and Price, MarketingContribution, SalesReps, SalesMediaPOS to the “managerial” form and draw marketing conclusions.

With the following code we are saving the formula: 

```{r}

formula <- Sales ~ Price + MarketingContribution + SalesReps + RetailMediaPOS

```



### Question 2

Inspecting the variables with descriptive statistics:

```{r}

#descriptive statistics can be checked with the following code
psych::describe(sales_data)


```

Inspecting the correlation matrix reveals that the sales variable is positively correlated with MarketingContribution, SalesReps and RetailMediaPOS.

```{r}

rcorr(as.matrix(sales_data[,c("Sales","Price","MarketingContribution","SalesReps", "RetailMediaPOS")]))

```


```{r}

#Since we have continuous variables, we use scatterplots to investigate the relationship between sales and each of the predictor variables.

# relationship of Price and Sales
ggplot(sales_data, aes(x = Price, y = Sales)) + geom_point(shape = 1) + geom_smooth(method = "lm", 
    fill = "gray", color = "lavenderblush3", alpha = 0.1) + theme_minimal()

#relationship of MarketingContribution and Sales
ggplot(sales_data, aes(x = MarketingContribution, y = Sales)) + geom_point(shape = 1) + geom_smooth(method = "lm", 
    fill = "gray", color = "lavenderblush3", alpha = 0.1) + theme_minimal()

#relationship of SalesReps and Sales
ggplot(sales_data, aes(x = SalesReps, y = Sales)) + geom_point(shape = 1) + geom_smooth(method = "lm", 
    fill = "gray", color = "lavenderblush3", alpha = 0.1) + theme_minimal()

#relationship of RetailMediaPOS
ggplot(sales_data, aes(x = RetailMediaPOS, y = Sales)) + geom_point(shape = 1) + geom_smooth(method = "lm", 
    fill = "gray", color = "lavenderblush3", alpha = 0.1) + theme_minimal()



```


The plots including the fitted lines from a simple linear model suggest that there might be a positive relationship between sales and the predictors. However, the relationships between sales and independent variables appear to be rather weak. It appears more that the effect of independent variables is decreasing with increasing budget spent on these marketing activities. 

Further steps include estimate of a multiple linear regression model in order to determine if linear specification fits the model.


### Question 3

First, we estimate the model with the lm() function. But before we can inspect and interpret the results, we need to test if there might be potential problems with our model specification.


```{r}

#Estimate linear model
linear_model <- lm(formula, data = sales_data)
summary(linear_model)

```


```{r}


#Outliers
sales_data$stud_resid <- rstudent(linear_model)
plot(1:nrow(sales_data),sales_data$stud_resid, ylim=c(-3.3,3.3)) #create scatterplot 
abline(h=c(-3,3),col="red",lty=2) #add reference lines


```

To check for outliers, we extract the studentized residuals from our model and test if there are any absolute values larger than 3. Since there are many residuals with absolute values larger than 3, we conclude that there are many outliers.

```{r}

#influential observation
plot(linear_model,4)
plot(linear_model,5)


```

To test for influential observations, we use Cook’s Distance. To identify influential observations, we should look at values above 1 in the first plot. It is easy to see that all of the Cook’s distance values are below the cutoff of 1.  In the second plot we should watch out for the outlying values at the upper right corner or at the lower right corner of the plot. Those spots are the places where cases can be influential against a regression line. In our example, both plots show that there are no influential cases.



```{r}

#Heteroscedasticity

plot(linear_model, 1)

#Breusch Pagan test
bptest(linear_model)

```


Next, we test if a linear specification appears feasible. We can look at the residuals plot to see if the linear specification is appropriate. This plot helps us to determine if error variances are equal, which is an assumption of the linear model. The red line is a smoothed curve through the residuals plot and if it deviates from the dashed grey horizontal line a lot, it means that the linear model specification is not a correct choice. In this example, the red line is rather far from the dashed grey line, so this assumption seems to be not met. 

Also, the residual variance (i.e., the spread of the values on the y-axis) should be similar across the scale of the fitted values on the x-axis, but we can observe a slight funnel shape, which indicates non-constant variances in the errors, so we can conduct Breusch Pagan test to confirm it. The null hypothesis for this test is that the error variances are equal, so the significant p-value <0,05 indicates that the error variances aren't equal, which indicates that the assumption of homoscedasticity is not met.


Next, we test if the residuals are approximately normally distributed using the Q-Q plot from the output and a Shapiro Wilk test.

```{r}

#Non-normally distributed errors

plot(linear_model,2)

shapiro.test(resid(linear_model))

```

To check for normal distribution of the residuals, we need to conduct a Shapiro-Wilk test, the null hypothesis for it is that the residuals are normally distributed. So the significant p-value <0,05 indicates that the error variances aren't equal, which indicates that the assumption of normally distributed error term is not met.

Correlation of errors: We actually wouldn’t need to test this assumption here since there is not natural order in the data.


```{r}

#Multicollinearity

rcorr(as.matrix(sales_data[,c("Price","MarketingContribution","SalesReps", "RetailMediaPOS")]))

plot(sales_data[,c("Price","MarketingContribution","SalesReps", "RetailMediaPOS")])

ggcorrmat(
  data = sales_data[,c("Price","MarketingContribution","SalesReps", "RetailMediaPOS")],
  matrix.type = "upper",
  colors = c("skyblue4", "white", "palevioletred4")
  #title = "Correlalogram of independent variables",
)

vif(linear_model)

```

To test for linear dependence of the regressors, we first test the bivariate correlations for any extremely high correlations (i.e., >0.8). We can see that bivariate correlations are not high among independent variable, so this assumption is met.

In the next step, we compute the variance inflation factor for each predictor variable. The values should be close to 1 and values larger than 4 indicate potential problems with the linear dependence of regressors, but in our model we don't see any such values. 

### Question 4

It appears that a linear model might not represent the data well. It rather appears that the effect of an additional Euro spend on different marketing activities is decreasing with increasing levels of advertising expenditures. Thus, we can assume decreasing marginal returns.  Plus, the assumptions of linear specification like absence of outliers, homoscedasticity and normally distributed residuals.

In this case, a multiplicative model might be a better representation of the data.

```{r}

log_reg <- lm(log(Sales) ~ log(Price) + log(MarketingContribution) + log(SalesReps) + log(RetailMediaPOS), data = sales_data)
summary(log_reg)


```

We can check again the assumptions to see if the multiplicative specification fits the model better and if we can interpret the results of this model.

```{r}

#Outliers
sales_data$stud_resid_1 <- rstudent(log_reg)
plot(1:nrow(sales_data),sales_data$stud_resid_1, ylim=c(-3.3,3.3)) #create scatterplot 
abline(h=c(-3,3),col="red",lty=2) #add reference lines

#influential observation
plot(log_reg,4)
plot(log_reg,5)

```

We can see that in comparison with the plot of the linear model, there are fewer outliers, namely values of studentized residuals more than 3 and less than -3. 


We check now if the assumption of constant variance is met, which was not the case in the linear specification.

```{r}

#Heteroscedasticity

plot(log_reg, 1)

#Breusch Pagan test
bptest(log_reg)

```

In the plot, we can now observe that the values are more equally distributed along the line. We also conduct Breusch Pagan test. The null hypothesis for this test is that the error variances are equal, so the significant p-value of 0.6857 indicates that we cannot reject the null hypothesis, so the error variances are equal, which indicates that the assumption of homoscedasticity is met for this model.

Next we check if the residuals are normally distributed, which wasn't the case in the linear model. 

```{r}
#Non-normally distributed errors

plot(log_reg,2)

shapiro.test(resid(log_reg))


```

The null hypothesis of this Shaprio Wilk test is that the residuals are normally distributed. Since the p-value of the test is 0.8898 we cannot reject the null hypothesis, so the residuals in this model are normally distributed unlike in the linear specification.

Looking at the added variable plots, we can conclude that each predictor appears to have a unique contribution to explaining the variance in the dependent variable. 


```{r}

#Non-linear relationships
avPlots(log_reg, col.lines = palette()[2])

```

And finally, we check for multicollinearity, 

```{r}

#Multicollinearity

vif(log_reg)


```
Here, all vif values are well below the cutoff, indicating that there are no problems with multicollinearity.

All assumption are met for the multiplicative model, log-log transformed model represents our data well, so we can proceed with the interpretation.

### Question 5

In a next step, we will investigate the results from the model.

```{r}

summary(log_reg)

```

The intercept tells us the amount of sales if all the marketing expenditures would be 0. The following variables have a significant influence on sales because for them the p-value is <0.05: log(Price), log(MarketingContribution), log(SalesReps), log(RetailMediaPOS).

The interpretation of the coefficients is as follows:
- A 1% increase in the sales price of the product leads to 1.3% decrease in sales holding other predictors constant.
- A 1% increase in promotion costs for including the product in leaflets distributed by the store leads to 0.3% increase in sales holding other predictors constant.
- A 1% increase in expenditures on in-store promotions managed by the sales reps leads to 0.25% increase in sales holding other predictors constant. 
- A 1% increase in advertising expenses for marketing placements at POS leads to 0.15% increase in sales holding other predictors constant.

We should always provide a measure of uncertainty that is associated with the estimates. You could compute the confidence intervals around the coefficients using the confint() function.

 
```{r}

confint(log_reg)

```

The results show that, for instance, with 95% confidence the true effect of the marketing contributions lies between 0.29 and 0.32.

Although the variables are measured on the same scale, you should still test the relative influence by inspecting the standardized coefficients that express the effects in terms of standard deviations.

 
```{r}

lm.beta(log_reg)

```
Here, we conclude that marketing contribution has the largest ROI followed by price (negative effect) and costs on sales reps.

Another significance test is the F-test. It tests the null hypothesis: H0:R-squared=0

This is equivalent to the following null hypothesis: H0:β1=β2=β3=βk=0

The result of the test is provided in the output above (F-statistic: 1464 on 4 and 1455 DF, p-value: <0,05). Since the p-value is smaller than 0.05, we reject the null hypothesis that all coefficients are zero.

Regarding the model fit, the R2 statistic tells us that approximately 80% of the variance can be explained by the model.

The results of the model and model fit can be also visualized with the following plots:

```{r}

ggcoefstats(x = log_reg, k = 3, title = "Sales predicted by marketing expenditures")

# Create the predictions (yhat) from the log-log model
sales_data$yhat <- predict(log_reg)

# Create a scatter plot of predicted vs. observed log(Sales)
ggplot(sales_data, aes(x = yhat, y = log(Sales))) +  
  geom_point(size = 2, shape = 1) +  # scatter plot of predicted vs observed log(Sales)
  scale_x_continuous(name = "Predicted log(Sales)") +  # label for the x-axis
  scale_y_continuous(name = "Observed log(Sales)") +  # label for the y-axis
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +  # identity line (45-degree)
  theme_minimal() +  # minimal theme
  labs(title = "Predicted vs. Observed log(Sales) - Log-Log Model")  # add title

```




### Question 6

Provide a description of your steps here! Below is a template for your equation:

$$log(Sales) = \beta_0 + \beta_1*log(Price)+ \beta_2*log(MarketingContribution)+\beta_3*log(SalesReps)+\beta_4*log(RetailMediaPOS)$$

$$log(Sales) = 6.474 + (-1.305)*log(350)+0.302*log(10000)+0.249*log(6000)+0.155+log(3000) $$

 
```{r}

Price <- 350
MarketingContribution <- 10000
SalesReps <- 6000
RetailMediaPOS <- 3000

# Get the coefficients from the model summary
intercept <- summary(log_reg)$coefficients[1,1]
coef_log_Price <- summary(log_reg)$coefficients[2,1]
coef_log_MarketingContribution <- summary(log_reg)$coefficients[3,1]
coef_log_SalesReps <- summary(log_reg)$coefficients[4,1]
coef_log_RetailMediaPOS <- summary(log_reg)$coefficients[5,1]

# Calculate the log(Sales) prediction using the log-log model formula
log_sales_prediction <- intercept + 
                        coef_log_Price * log(Price) + 
                        coef_log_MarketingContribution * log(MarketingContribution) + 
                        coef_log_SalesReps * log(SalesReps) + 
                        coef_log_RetailMediaPOS * log(RetailMediaPOS) 
                  

# Exponentiate to get the predicted sales
sales_prediction <- exp(log_sales_prediction)

# Output the predicted sales
sales_prediction


```

The predicted sales with the given marketing expenditures are 151 units.

### Question 7

First we visualize the relationship between the 3 variables and then formally test the effect in the model.

```{r}

#boxplots of sales by regions
ggplot(sales_data, aes(x = Region, y = Sales)) +
  geom_boxplot(fill = "gray", color = "lavenderblush3") +  # Fill and color matching scatter plot
  labs(title = "Sales by Region", x = "Region", y = "Sales") +
  theme_minimal() +
  theme(legend.position = "none")

# Transform data to log scale for the scatter plot
sales_data$log_Sales <- log(sales_data$Sales)
sales_data$log_Price <- log(sales_data$Price)

# Create a scatter plot of log(Sales) vs log(Price) by Region
ggplot(sales_data, aes(x = log_Price, y = log_Sales, color = Region)) +
  geom_point(alpha = 0.6) +   # Scatter points with some transparency
  geom_smooth(method = "lm", se = FALSE, aes(color = Region)) +  # Add regression lines
  labs(
    title = "Log-Log Relationship Between Sales and Price by Region",
    x = "Log(Price)",
    y = "Log(Sales)",
    color = "Region"
  ) +
  theme_minimal() +
  theme(legend.position = "top")

```

From the inspection of the plot, we can already see that there are only very slight differences in price sensitivity between regions. We can see the more exact result in the model.


```{r}

log_reg_with_region <- lm(log(Sales) ~ log(Price) + log(MarketingContribution) + log(SalesReps) + log(RetailMediaPOS) + Region + log(Price)*Region, data = sales_data)

summary(log_reg_with_region)


```
From inspecting the model, we can conclude that region indeed has an effect on price sensitivity.

The coefficient for RegionSuburban indicates that in the Suburban region the sales are expected to be 1.09% higher than in the Rural region. This effect is significant (p<0.05). Similarly, for the RegionUrban sales are expected to be 1.29% higher than in the Rural region, and this effect is also statistically significant (p<0.05).

The interaction effect: The coefficient log(Price) of -1.167 demonstrates the base price sensitivity in the rural region. This means that in the rural region a 1% increase in price will lead to a 1.167% decrease in sales. In the suburban region, customers are more price sensitive by 0.187. This means that a 1% increase in price will lead to (-1.167-0.187=) 1.354% decrease in sales. In urban region, people are the most price sensitive, by 0.22 more than in the rural region. A 1% increase in price will lead to (-1.167-0.22 =) 1.387% decrease in sales. 

This result could indicate that in urban areas consumers have access to more alternatives and hence, react stronger to price changes. 


### Assignment B

As a marketing manager of a music streaming service, you are set the task to derive insights from data using different quantitative analyses.   

The following variables are available to you:

The data set contains the following variables:

* isrc = unique song id
* artist_id = unique artist ID
* streams = the number of streams of the song received globally between 2017-2021
* weeks_in_charts = the number of weeks the song was in the top200 charts in this period
* n_regions = the number of markets where the song appeared in the top200 charts
* danceability
* energy
* speechiness
* instrumentalness
* liveness
* valence
* tempo
* song_length = the duration of the song (in minutes)
* song_age = the age of the song (in weeks since release)
* explicit = indicator for explicit lyrics
* n_playlists = number of playlists a song is featured on
* sp_popularity = the Spotify popularity index of an artist
* youtube_views = the number of views the song received on YouTube
* tiktok_counts = the number of Tiktok views the song received on TikTok
* ins_followers_artist = the number of Instagram followers of the artist
* monthly_listeners_artist = the number of monthly listeners of an artist
* playlist_total_reach_artist = the number of playlist followers of the playlists the song is on
* sp_fans_artist = the number of fans of the artist on Spotify
* shazam_counts = the number of times a song is shazamed
* artistName = name of the artist
* trackName = name of the song
* release_date = release date of song
* genre = genre associated with the song
* label = music label associated with the song
* top10 = indicator variable, indicating if a song made it to the top10

1. Build and estimate a classification model (i.e., logistic regression) to explain the success of songs in terms of the chart position (i.e., if a song made it to the top 10 or not). This means, the variable "top10"" is your dependent variable. As independent (explanatory) variables, you should include the variables "weeks_in_charts", "song_age" and "label". In addition to these 3 variables you should identify 5 more variables that have a significant effect on the chart performance. Please visualize the relationship between the top10 variable and the independent variables using appropriate plots and interpret the model coefficients.

### Data analysis

### Load data

```{r load_data_112, warning=FALSE, message=FALSE}
library(ggplot2)
library(psych)
library(dplyr)
options(scipen = 999)
set.seed(123)
music_data <- read.csv2("https://raw.githubusercontent.com/WU-RDS/RMA2022/main/data/music_data_group.csv",
                        sep = ";", header = TRUE, dec = ",")
music_data$genre <- as.factor(music_data$genre)
music_data$label <- as.factor(music_data$label)
str(music_data)
```

### Question 1


For this model, we need to consider the logistic function, so the final mathematical representation (with three main predictors of interest so far) would look as follows:


$$f(\mathbf{X}) = P(y_i = 1) = \frac{1}{1
+ e^{-(\beta_0 + \beta_1 * x_{1,i} + \beta_2 * x_{2,i} +\beta_3 *
x_{3,i})}}$$

where β0 is the intercept coefficient, and β1, β2, and β3represent the parameters of our model: weeks in charts, age of song, and label.

We should create the model using glm() and have a look at the summary

```{r}
mult_logit_model <- glm(top10 ~ weeks_in_charts + song_age + label, family = binomial (link = 'logit'), data = music_data)
summary(mult_logit_model)
```





```{r}
confint(mult_logit_model)

```

From the summary of the model we can see that weeks in charts, age of song, and label can be used to predict if a song will end up in top-10 or not. We can also assess the model fit:

```{r}

logisticPseudoR2s <- function(LogModel) {
  dev <- LogModel$deviance 
  nullDev <- LogModel$null.deviance 
  modelN <- length(LogModel$fitted.values)
  R.l <-  1 -  dev / nullDev
  R.cs <- 1- exp(-(nullDev - dev) / modelN)
  R.n <- R.cs / ( 1 - (exp(-(nullDev / modelN))))
  cat("Pseudo R^2 for logistic regression\n")
  cat("Hosmer and Lemeshow R^2  ", round(R.l, 3), "\n")
  cat("Cox and Snell R^2        ", round(R.cs, 3), "\n")
  cat("Nagelkerke R^2           ", round(R.n, 3),    "\n")
}
#Inspect Pseudo R2s
logisticPseudoR2s(mult_logit_model)

```

To make conclusions about the effect that predictors have on success, we should convert the log-odds ratios to odds ratios using exp() function:

```{r}
exp(coef(mult_logit_model))
```


The results tell us, for example, that for each extra week in charts, the odds of the song to be in the top 10 are 1.0126 times higher, or equivalently, the likelihood of the outcome is increased by 1.26%. Or songs from Universal are 2.38 times more likely to appear in the top-10 chart in comparison with songs from independent labels, or equivalently, the likelihood of the outcome is increased by 138.5% compared to independent labels. 

We should visualize the relationship between IVs and DV:

```{r}
#Relationship of weeks in charts and top10

ggplot(music_data, aes(weeks_in_charts, top10)) +  
  geom_point(shape = 1) +
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"), 
              se = FALSE, color = "lavenderblush3") + theme_minimal()
```



```{r}
#Relationship of song age and top10

ggplot(music_data, aes(song_age, top10)) +  
  geom_point(shape = 1) +
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"), 
              se = FALSE, color = "lavenderblush3") + theme_minimal()
```



```{r}
#Relationship of label and top10

library(forcats)
labels <- as.factor(c("Warner Music", "Sony Music", "Independent", "Universal Music"))
top10_predictions <- data.frame(pred = 
                                  predict(glm(top10 ~ label, data = music_data), 
                                          data.frame(label = labels), type = "response"), 
                                label = labels)
top10_counts <- table(music_data$top10, music_data$label)
top10_share <- prop.table(top10_counts, margin = 2)
data.frame(top10_share) |>
  filter(Var1 == 1) |> 
  left_join(top10_predictions, by = c("Var2" = "label")) |>
  dplyr::rename(`Top 10 share` = Freq) |>
  ggplot(aes(fct_reorder(Var2, `Top 10 share`), `Top 10 share`)) +
  geom_bar(stat = 'identity', fill = "lavenderblush3") +
  geom_point(aes(x = Var2, y = pred), color = 'red4') +
  theme_minimal() +
  theme(axis.title.x = element_blank())
```

To find out which other variables might have a significant effect on the chart performance, we can either load variables one-by-one manually or use a step-wise approach. For the latter, we basically need a model to start with (usually it’s a “null” model, however, we already have a model that works for us, i.e., mult_logit_model) and the most loaded model that includes all the variables (we will only drop all character and date variables). Let’s create it in the next step (please note that we already drop some variables that potentially might be influenced if a song appears in top-10: streams, sp_popularity, n_regions, etc.)

```{r}
music_data$explicit <- factor(music_data$explicit, 
                                    levels = c(0,1), labels = c("not explicit", "explicit"))

full_model <- glm(top10 ~ weeks_in_charts + song_age + label + #our basic model. Next we add the rest of the variables to it:
                    danceability + energy + speechiness + instrumentalness + liveness + valence + tempo +
                    song_length + explicit + n_playlists + genre,
                  family = binomial(link = 'logit'), data = music_data)
```

Let’s have a look at the fullest model possible:

```{r}
summary(full_model)
```

We can pick five significant factors from the summary above. For example, we can proceed with the model as follows:

```{r}
final_model <- glm(top10 ~ weeks_in_charts + song_age + label + #our basic model. Next we add the rest of the variables to it:
                   danceability + liveness + tempo + song_length + n_playlists,
                   family = binomial(link = 'logit'), data = music_data)

summary(final_model)
```

```{r}
logisticPseudoR2s(final_model)
```

We can see from the table above that the model explains 50% of the variability in the likelihood of the song being in top 10 (according to Nagelkerke R-squared).

```{r}
exp(coef(final_model))
```

Interpretation: The results tell us, for example, that for each one minute increase in song length, the odds of the song to be in the top 10 are 0.73 times lower, or equivalently, the likelihood of the song to be in top 10 is decreased by 27%. For each one unit increase in danceability, the odds of the song to be in top 10 are 1.02 times higher, or equivalently, the likelihood of the song being in top 10 is increased by 2%.

Alternatively, average partial effect as means of model interpretation can be used:
```{r}
library(mfx)
# Average partial effect
logitmfx(final_model, data = music_data, atmean = FALSE)
```


Interpretation of average partial effects: If a song is one week older, the probability of this song appearing in top-10 chart decreases by 0.01 percentage points.

If we still want to choose a parsimonious model using step-wise comparisons, we can do it as follows: the function below takes the “base” model, adds variables from the fullest model one-by-one to it, and shows the new models’ performance.

AIC values can be compared, we want the model with lowest possible AIC. As a second measure for variable selection, we can use the pseudo R-squared, that gives a value for explanatory power of the model.

```{r}
step(mult_logit_model, #our base model
     scope = list(upper = full_model), 
     direction = "both",
     test = "Chisq",
     data = music_data)
```



## Assignment 4

### Assignment A

The data-set `influencer` contains the following variables to assess the success of influencer marketing campaigns:

  - `follower_count`: the number of followers of the influencer
  - `engagement_rate`: the engagement rate measured as the number of interations divided by the influencers reach
  - `content_quality`: an index measuring the quality of the influencer's posts
  - `post_frequency`: the number of times the influencer posted about the brand
  - `sales`: the number of units sold during the campaign
  - `brand_awareness`: an index measuring how many people became aware of the brand during the campaign

**Task Instructions**

#### Task 1: Omitted Variable Bias

```{r}
influencer <- readr::read_csv("https://github.com/WU-RDS/MA2024/raw/refs/heads/main/data/influencer.csv")
str(influencer)
set.seed(1234)
```

We are interested in modeling the sales during the campaigns. 

a) Draw a DAG that shows how you would model the causal influence of of `follower_count` on `sales` (feel free to draw it by hand and insert an image. You can add images with `![](./IMAGE.png)`)


```{r}

base_model <- lm(sales ~ follower_count, data = influencer)
summary(base_model)

```

```{r}

engagement_model <- lm(sales ~ follower_count + engagement_rate, data = influencer)
summary(engagement_model)

```
```{r}

frequency_model <- lm(sales ~ follower_count + post_frequency, data = influencer)
summary(frequency_model)

```

```{r}

content_model <- lm(sales ~ follower_count + content_quality, data = influencer)
summary(content_model)

```

```{r}

awareness_model <- lm(sales ~ follower_count + brand_awareness, data = influencer)
summary(awareness_model)

```

We ran several models, starting with a base model with one independent variable follower_count and sales as dependent variable. Then we added different variables in this model to see if some of them add explanatory power, and it's the case only for the variable engagement_rate. The effect of follower count on sales increases when engagement rate is added to the model from 0.9 to 1.9, suggesting that engagement rate is acting as a confounder, influencing both follower_count and sales.


b) Describe briefly what roles each of the variables in you DAG play (mediator, moderator, collider)

```{r}

#Engagement rate is a confounder

library(DiagrammeR)

mermaid("
graph LR
   engagement_rate --> follower_count
   engagement_rate --> sales
   follower_count --> sales
")


```


c) Calculate the correlation coefficients of the variables you expect to be correlated based on your answer in a). Briefly explain any changes (if any) you would like to make to your DAG based on the results.

```{r}

# Load necessary libraries
library(ggplot2)
library(corrplot)


#Variable selection
variables <- influencer[, c("follower_count", "sales", "engagement_rate", 
                              "brand_awareness", "content_quality", "post_frequency")]

# Calculate correlation matrix
cor_matrix <- cor(variables, use = "complete.obs", method = "pearson")

# Print the correlation matrix
print(cor_matrix)

# Visualize the correlation matrix using corrplot
corrplot(cor_matrix, method = "circle", type = "upper", 
         tl.col = "black", tl.srt = 45, addCoef.col = "black")

```


d) Run and interpret the appropriate model given your answers in a) - c) (hint: you don't need anything we did not do in class)

```{r}

#Confounder - engagement rate

base_model <- lm(sales ~ follower_count, data = influencer)
summary(base_model)

engagement_model <- lm(sales ~ follower_count + engagement_rate, data = influencer)
summary(engagement_model)

```


All the coefficients have a p-value lower than 0.05, which means they are all significant, so we can proceed with the interpretations. 

The coefficient for follower count is now 1.99, meaning for each additional follower, sales increase by 1.99 units, holding another predictor constant. This is higher than the previous value of 0.9 in the base model. After adjusting for engagement rate, the effect of follower count on sales becomes stronger. The coefficient for engagement rate is 2.1, meaning for each unit increase in engagement rate, sales increase by 2.1 units, holding another predictor constant.

The R-squared of the model increased from 0.13 in the base model to 0.5 in the model including engagement rate. So, this model explains 50% of the variance in sales. 

#### Task 2: Selection Bias

```{r}
influencer2 <- readr::read_csv("https://github.com/WU-RDS/MA2024/raw/refs/heads/main/data/influencer2.csv")
str(influencer2)
```

The data-set `influencer2` contains data from an experiment run by a company to assess whether their selection criteria for influencer campaigns are suitable to assess the impact authenticity of their posts have on sales. Instead of their usual selection criteria they select influencers at random to check if their usual selection criteria change the relationship between authenticit and sales. The data contains the following variables:

 - `sales_impact`: the measured sales impact of an influencer
 - `authenticity`: an index measuring the perceived authenticity of the influencer's posts
 - `selected`: an indicator of whether this influencer would have been selected according to the companies usual criteria

Since their usual selection criteria include the expected sales impact of a campaign as well as the authenticity index the company suspects that the selection is a collider.

a) Please estimate and interpret a model that shows the causal impact of authenticity on sales assuming the company is correct and their selection is a collider

```{r}
# provide your code here 

model_correct <- lm(sales_impact ~ authenticity, influencer2)
model_collider <-  lm(sales_impact ~ authenticity + selected, influencer2)

summary(model_correct)
summary(model_collider)

```

b) Draw the DAG for the model (feel free to draw it by hand and insert an image)

```{r}

#collider
library(DiagrammeR)

## Model the situation assumed by the company (=collider) 
mermaid("
graph LR
   authenticity--> select
   sales--> select
   authenticity-->sales
")

```

#### Task 3: Moderation

```{r}
in_app_purchases <- readr::read_csv("https://github.com/WU-RDS/MA2024/raw/refs/heads/main/data/in_app_purchases.csv")
str(in_app_purchases)
```


The data-set `in_app_purchases` includes data on a marketing campaign aimed at boosting in-app purchases. It contains the variables:

- `in_app`: the number of in-app purchases of a user
- `gender`: the gender of the user
- `campaign_exposure`: indicator whether the user was exposed to the campaign

a) Provide an appropriate visualization showing whether the effect of the campaign is moderated by gender

```{r}

library(ggplot2)

ggplot(in_app_purchases, aes(x = campaign_exposure, y = in_app_purchases, fill = gender)) +
  geom_boxplot(alpha = 0.7, position = position_dodge(width = 0.8)) +  # Adjusted dodge for separation
  labs(
    title = "In-App Purchases by Campaign Exposure and Gender",
    x = "Campaign Exposure",
    y = "In-App Purchases",
    fill = "Gender"
  ) +
  theme_minimal() +
  theme(
    legend.position = "top",
    axis.text = element_text(size = 12),      # Improves text size
    plot.title = element_text(size = 16, face = "bold")  # Enhances title appearance
  )


```

From the boxplots we can already see that gender moderates the effect of the campaign. This campaign works much better on women than on men. We can check it formally by running the regression and looking at the coefficients. 


a) Calculate and interpret all of the coefficients in the appropriate model given your answer in a)

```{r}

moderation_gender <- lm(in_app_purchases ~ campaign_exposure*gender, data = in_app_purchases)
summary(moderation_gender)

```

From the model we can see that all the coefficients are significant (p<0.05), so we can proceed with interpreting them.

campaign_exposureNot Exposed - When the female customers are not exposed to the campaign, their in app purchases fall by 7.8 units in comparison with customers exposed to the campaign.
genderMale - Men who are exposed to the campaign make 4.04 fewer in-app purchases on average compared to women who are exposed to the campaign.
campaign_exposureNot Exposed:genderMale - Men are less negatively affected by not being exposed to the campaign compared to women. 

-7.8 + 3.9 = -3.9: If men are not exposed to the campaign, their in app purchases fall by 3.9 units in comparison with decrease of 7.8 among women. 


#### Task 4: The IKEA effect


```{r}
ikea <- readr::read_csv("https://github.com/WU-RDS/MA2024/raw/refs/heads/main/data/ikea.csv")
str(ikea)
```

The IKEA effect describes a psychological process through which consumers value products more because they feel a sense of accomplishment after assembling the something themselves. Using the following data, analyze whether you can replicate these results. The data was created through an experiment in which some consumers were randomly given fully assembled furniture while others had to assemble it themselves (it is otherwise the same furniture; variable: `self_assembly`). The researchers asked consumers about their sense of accomplishment (variable: `accomplishment`) and how much they value the final product (variable: `valuation`). Discuss the relative magnitude of the direct and mediated effect of self assembly.

```{r}
# provide your code here 

#Mediation - engagement rate

#Step 1: Run regression just with X and Y
total_effect_ikea <- lm(valuation ~ self_assembly, data = ikea)

#Step 2: Run regression just with M1
X_on_M_ikea <- lm(accomplishment ~ self_assembly, data = ikea)


# Step 3: Run regression with Y on X and M1
avg_direct_effect_M_ikea <- lm(valuation ~ self_assembly + accomplishment, data = ikea)

summary(total_effect_ikea)
summary(X_on_M_ikea)
summary(avg_direct_effect_M_ikea)

# Indirect effect: self_assembly -> accomplishment -> valuation
indirect_effect <- coef(X_on_M_ikea)["self_assembly"] * coef(avg_direct_effect_M_ikea)["accomplishment"]

# Print the indirect effect
indirect_effect


```


The coefficients in all 3 models are significant (p<0.05), so we can proceed with the interpretations. 

First model with just the effect of self assembly on valuation: 11.54 (p<0.05) If consumers assemble the product themselves (as opposed to receiving it fully assembled), their valuation of the product increases by 11.54 units.

Second model with the effect of the self assembly on the sense of accomplishment: If the furniture is self assembled, it increases the sense of accomplishment by approximately 2.12 units in comparison with furniture that doesn't have to be assembled. 

Third model with the effect of both self assembly and accomplishment on valuation: After including the accomplishment variable in the model, self assembly still has a positive effect on valuation, but now the effect is reduced compared to the total effect in the first step. The reduction suggests part of the effect of self assembly on valuation is mediated by accomplishment.

As the next step, we calculate the indirect effect of the self assembly on valuation through sense of accomplishment by multiplying the coefficients from the first and second models: 2.122 * 2.889 = 6.13

The mediated effect via accomplishment (=6.13) is slightly larger than the direct effect (5.41), suggesting that the IKEA effect (the increased valuation due to self assembly) is largely driven by the sense of accomplishment that consumers feel when assembling the product themselves.
